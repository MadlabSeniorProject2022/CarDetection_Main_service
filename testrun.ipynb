{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LICENSE.md', 'tools', '.DS_Store', 'requirements.txt', 'deploy', 'figure', 'database.py', 'utils', 'models', '__pycache__', 'cfg', 'cld1500.pt', 'yolov7.pt', 'README.md', 'frameSpliter.py', 'lpd_model.py', 'main_model.py', 'static', 'scripts', 'app.py', 'inference', 'templates', '.ipynb_checkpoints', 'LPD150.pt', 'traced_model.pt', 'testrun.ipynb', 'hubconf.py', '.git', 'data', 'runs']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.24.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/base.py:329: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.24.2 when using version 1.1.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import main_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR ðŸš€ 2022-12-7 torch 1.12.1 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n",
      "Model Summary: 306 layers, 36905341 parameters, 6652669 gradients\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      " Convert model to Traced-model... \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/thisisthanick/Library/Mobile Documents/com~apple~CloudDocs/Dev/CarDetection_WebApp/testrun.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/thisisthanick/Library/Mobile%20Documents/com~apple~CloudDocs/Dev/CarDetection_WebApp/testrun.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m main_model\u001b[39m.\u001b[39;49mrun(\u001b[39m'\u001b[39;49m\u001b[39m./static/files/frames/use007-730-2\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m./static/files/car/use007-730-2\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m./static/files/license/use007-730-2\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Dev/CarDetection_WebApp/main_model.py:221\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(source, file_directory, lp_directory)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m (source, file_directory, lp_directory):\n\u001b[1;32m    220\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 221\u001b[0m         detect(source, file_directory, lp_directory)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Dev/CarDetection_WebApp/main_model.py:54\u001b[0m, in \u001b[0;36mdetect\u001b[0;34m(source, file_directory, lp_directory, save_img)\u001b[0m\n\u001b[1;32m     51\u001b[0m imgsz\u001b[39m=\u001b[39m check_img_size(imgsz, s\u001b[39m=\u001b[39mstride)  \u001b[39m# check img_size\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m trace:\n\u001b[0;32m---> 54\u001b[0m     model \u001b[39m=\u001b[39m TracedModel(model, device, \u001b[39m640\u001b[39;49m)\n\u001b[1;32m     56\u001b[0m \u001b[39mif\u001b[39;00m half:\n\u001b[1;32m     57\u001b[0m     model\u001b[39m.\u001b[39mhalf()  \u001b[39m# to FP16\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Dev/CarDetection_WebApp/utils/torch_utils.py:362\u001b[0m, in \u001b[0;36mTracedModel.__init__\u001b[0;34m(self, model, device, img_size)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtraced \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    360\u001b[0m rand_example \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m, img_size, img_size)\n\u001b[0;32m--> 362\u001b[0m traced_script_module \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mjit\u001b[39m.\u001b[39;49mtrace(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel, rand_example, strict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    363\u001b[0m \u001b[39m#traced_script_module = torch.jit.script(self.model)\u001b[39;00m\n\u001b[1;32m    364\u001b[0m traced_script_module\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mtraced_model.pt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/jit/_trace.py:750\u001b[0m, in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[39mreturn\u001b[39;00m func\n\u001b[1;32m    749\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(func, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m--> 750\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    751\u001b[0m         func,\n\u001b[1;32m    752\u001b[0m         {\u001b[39m\"\u001b[39;49m\u001b[39mforward\u001b[39;49m\u001b[39m\"\u001b[39;49m: example_inputs},\n\u001b[1;32m    753\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    754\u001b[0m         check_trace,\n\u001b[1;32m    755\u001b[0m         wrap_check_inputs(check_inputs),\n\u001b[1;32m    756\u001b[0m         check_tolerance,\n\u001b[1;32m    757\u001b[0m         strict,\n\u001b[1;32m    758\u001b[0m         _force_outplace,\n\u001b[1;32m    759\u001b[0m         _module_class,\n\u001b[1;32m    760\u001b[0m     )\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    763\u001b[0m     \u001b[39mhasattr\u001b[39m(func, \u001b[39m\"\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    764\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m, torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mModule)\n\u001b[1;32m    765\u001b[0m     \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    766\u001b[0m ):\n\u001b[1;32m    767\u001b[0m     \u001b[39mreturn\u001b[39;00m trace_module(\n\u001b[1;32m    768\u001b[0m         func\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m,\n\u001b[1;32m    769\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39mforward\u001b[39m\u001b[39m\"\u001b[39m: example_inputs},\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    776\u001b[0m         _module_class,\n\u001b[1;32m    777\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/jit/_trace.py:992\u001b[0m, in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, strict, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    981\u001b[0m                 _check_trace(\n\u001b[1;32m    982\u001b[0m                     check_inputs,\n\u001b[1;32m    983\u001b[0m                     func,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    989\u001b[0m                     _module_class,\n\u001b[1;32m    990\u001b[0m                 )\n\u001b[1;32m    991\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 992\u001b[0m                 _check_trace(\n\u001b[1;32m    993\u001b[0m                     [inputs],\n\u001b[1;32m    994\u001b[0m                     func,\n\u001b[1;32m    995\u001b[0m                     check_trace_method,\n\u001b[1;32m    996\u001b[0m                     check_tolerance,\n\u001b[1;32m    997\u001b[0m                     strict,\n\u001b[1;32m    998\u001b[0m                     _force_outplace,\n\u001b[1;32m    999\u001b[0m                     \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   1000\u001b[0m                     _module_class,\n\u001b[1;32m   1001\u001b[0m                 )\n\u001b[1;32m   1002\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m     torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39m_trace\u001b[39m.\u001b[39m_trace_module_map \u001b[39m=\u001b[39m old_module_map\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/jit/_trace.py:525\u001b[0m, in \u001b[0;36m_check_trace\u001b[0;34m(check_inputs, func, traced_func, check_tolerance, strict, force_outplace, is_trace_module, _module_class)\u001b[0m\n\u001b[1;32m    521\u001b[0m             all_ok \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    523\u001b[0m     \u001b[39mreturn\u001b[39;00m all_ok\n\u001b[0;32m--> 525\u001b[0m traced_outs \u001b[39m=\u001b[39m run_mod_and_filter_tensor_outputs(traced_func, inputs, \u001b[39m\"\u001b[39;49m\u001b[39mtrace\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    526\u001b[0m fn_outs \u001b[39m=\u001b[39m run_mod_and_filter_tensor_outputs(func, inputs, \u001b[39m\"\u001b[39m\u001b[39mPython function\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    527\u001b[0m \u001b[39mif\u001b[39;00m compare_outputs(traced_outs, fn_outs, \u001b[39m\"\u001b[39m\u001b[39mPython function\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/torch/jit/_trace.py:443\u001b[0m, in \u001b[0;36m_check_trace.<locals>.run_mod_and_filter_tensor_outputs\u001b[0;34m(mod, inputs, running_what)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_mod_and_filter_tensor_outputs\u001b[39m(mod, inputs, running_what):\n\u001b[1;32m    442\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m         outs \u001b[39m=\u001b[39m wrap_retval(mod(\u001b[39m*\u001b[39;49m_clone_inputs(inputs)))\n\u001b[1;32m    444\u001b[0m         outs \u001b[39m=\u001b[39m [out \u001b[39mfor\u001b[39;00m out \u001b[39min\u001b[39;00m outs \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out, torch\u001b[39m.\u001b[39mTensor)]\n\u001b[1;32m    445\u001b[0m         \u001b[39mreturn\u001b[39;00m outs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_model.run('./static/files/frames/frame98', './static/files/car/frame98', './static/files/license/frame98')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "YOLOR ðŸš€ 2022-12-7 torch 1.12.1 CPU\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Summary: 314 layers, 36481772 parameters, 6194944 gradients, 103.2 GFLOPS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "RepConv.fuse_repvgg_block\n",
      "IDetect.fuse\n",
      " Convert model to Traced-model... \n",
      " traced_script_module saved! \n",
      " model is traced! \n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No images or videos found in /Users/thisisthanick/Library/Mobile Documents/com~apple~CloudDocs/Dev/CarDetection_WebApp/static/files/car/frame98. Supported formats are:\nimages: ['bmp', 'jpg', 'jpeg', 'png', 'tif', 'tiff', 'dng', 'webp', 'mpo']\nvideos: ['mov', 'avi', 'mp4', 'mpg', 'mpeg', 'm4v', 'wmv', 'mkv']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/thisisthanick/Library/Mobile Documents/com~apple~CloudDocs/Dev/CarDetection_WebApp/testrun.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/thisisthanick/Library/Mobile%20Documents/com~apple~CloudDocs/Dev/CarDetection_WebApp/testrun.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlpd_model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/thisisthanick/Library/Mobile%20Documents/com~apple~CloudDocs/Dev/CarDetection_WebApp/testrun.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m lpd_model\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mstatic/files/car/frame98/\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstatic/files/license/frame98/\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Dev/CarDetection_WebApp/lpd_model.py:156\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(source, file_directory)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m (source, file_directory):\n\u001b[1;32m    155\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 156\u001b[0m         \u001b[39mreturn\u001b[39;00m detect(source, file_directory)\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Dev/CarDetection_WebApp/lpd_model.py:63\u001b[0m, in \u001b[0;36mdetect\u001b[0;34m(source, file_directory, save_img)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m# Set Dataloader\u001b[39;00m\n\u001b[1;32m     62\u001b[0m vid_path, vid_writer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m dataset \u001b[39m=\u001b[39m LoadImages(source, img_size\u001b[39m=\u001b[39;49mimgsz, stride\u001b[39m=\u001b[39;49mstride)\n\u001b[1;32m     65\u001b[0m \u001b[39m# Get names and colors\u001b[39;00m\n\u001b[1;32m     66\u001b[0m names \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mmodule\u001b[39m.\u001b[39mnames \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model, \u001b[39m'\u001b[39m\u001b[39mmodule\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m model\u001b[39m.\u001b[39mnames\n",
      "File \u001b[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Dev/CarDetection_WebApp/utils/datasets.py:154\u001b[0m, in \u001b[0;36mLoadImages.__init__\u001b[0;34m(self, path, img_size, stride)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnf \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNo images or videos found in \u001b[39m\u001b[39m{\u001b[39;00mp\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m \\\n\u001b[1;32m    155\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mSupported formats are:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mimages: \u001b[39m\u001b[39m{\u001b[39;00mimg_formats\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mvideos: \u001b[39m\u001b[39m{\u001b[39;00mvid_formats\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: No images or videos found in /Users/thisisthanick/Library/Mobile Documents/com~apple~CloudDocs/Dev/CarDetection_WebApp/static/files/car/frame98. Supported formats are:\nimages: ['bmp', 'jpg', 'jpeg', 'png', 'tif', 'tiff', 'dng', 'webp', 'mpo']\nvideos: ['mov', 'avi', 'mp4', 'mpg', 'mpeg', 'm4v', 'wmv', 'mkv']"
     ]
    }
   ],
   "source": [
    "import lpd_model\n",
    "\n",
    "lpd_model.run(\"static/files/car/frame98/\", \"static/files/license/frame98/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
